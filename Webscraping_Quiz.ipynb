{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Webscraping Quiz"
      ],
      "metadata": {
        "id": "8ATOcKzNCdKb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to webscrape: https://quotes.toscrape.com/\n",
        "You will have an expanding set of tasks that starts with collecting a"
      ],
      "metadata": {
        "id": "4SSDvuZkCx17"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oIX650ZKj-OC"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "6D0gnd32Cv9K"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://quotes.toscrape.com/\"\n",
        "resp = requests.get(url, timeout=20)\n",
        "find_text = soup.find_all('div', class_ = \"quote\")\n",
        "all_divs = find_text[1].find_all('div')\n",
        "all_divs[0].get_text(strip = True)\n",
        "#print(find_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "kZB-g_3ukAtK",
        "outputId": "83b141c6-c96c-45ac-e02c-a22690592130"
      },
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "list index out of range",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2366517612.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfind_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'div'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"quote\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mall_divs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_text\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'div'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mall_divs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m#print(find_text)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "i inspected the website and found the keyworks of the first quote are quote and it was in a div ike the all the other quotes. then i gathered the index of the quote and printed it."
      ],
      "metadata": {
        "id": "FA_5PlkukAXk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tasks:"
      ],
      "metadata": {
        "id": "8SvPSKP6C0IM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4 Levels:<br>\n",
        "Level 1 (B): Grab the first quote from the front page. Print out the quote. <br>\n",
        "Level 2 (B+): Grab all the quotes from the front page. Store quotes in a list or dictionary (see Level 4).<br>\n",
        "Level 3 (A): Grab all the quotes from all 10 pages of the website. Store quotes in a list or dictionary (see Level 4)<br>\n",
        "Level 4 (A+): Create a dictionary where the keys are people and the values are a list of quotes by that person. Use all quotes from Level 3<br>\n",
        "\n",
        "Full points given to programmatic solutions that effectively use BeautifulSoup.<br><br>\n",
        "Use an intelligent combination of code and text blocks to demonstrate your understanding of using Python and BeautifulSoup to solve this problem."
      ],
      "metadata": {
        "id": "YGffYnHZBgRP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://quotes.toscrape.com/\"\n",
        "resp = requests.get(url, timeout=20)\n",
        "print(resp.status_code)\n",
        "print(resp.text[:500])\n",
        "soup = BeautifulSoup(resp.text, \"html.parser\")\n",
        "find_quote = []\n",
        "find_quote = soup.find_all('div', class_ = \"quote\")\n",
        "for quote in find_quote:\n",
        "  all_divs = quote.find_all('div')\n",
        "  quote_text = all_divs[0].get_text(strip = True)\n",
        "  print(find_quote)\n",
        ""
      ],
      "metadata": {
        "id": "j_D3xMo8C2kL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "i did the same thing but instead i looped through all of the quotes to gather them all and created a list classed find_quote and added the quotes to that list."
      ],
      "metadata": {
        "id": "hSJ1I-NxqpUd"
      }
    }
  ]
}